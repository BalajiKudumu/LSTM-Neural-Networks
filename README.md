# Next_Word_Prediction_LSTM_RNN

## LSTM Neural Network for Next Word Prediction

## Overview

This repository explains how an **LSTM (Long Short-Term Memory)** neural network works and how it can be applied for **next-word prediction**.

LSTMs are a type of Recurrent Neural Network (RNN) designed to model long-range dependencies in sequential data, such as text.

---

## ğŸ§  LSTM Architecture

An LSTM unit consists of:

* **Cell state (c\_t)** â€” memory of the network
* **Hidden state (h\_t)** â€” output at time `t`
* **Gates**:

  * Forget gate `f_t`: What to discard
  * Input gate `i_t`: What to update
  * Output gate `o_t`: What to output

### LSTM Equations

Given input `x_t` at time `t`:

``
f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)

i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)

cÌƒ_t = tanh(W_c Â· [h_{t-1}, x_t] + b_c)

c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ cÌƒ_t

o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)

h_t = o_t âŠ™ tanh(c_t)
``

Where:

* `Ïƒ` = sigmoid activation
* `tanh` = hyperbolic tangent activation
* `âŠ™` = element-wise multiplication
* `W_f, W_i, W_c, W_o` = learned weights
* `b_f, b_i, b_c, b_o` = biases

--

## âœ‰ï¸ Next-Word Prediction Example

**Input sequence:**

``
I love to
``

**Goal:** Predict next word.

1ï¸âƒ£ Convert words to embeddings â†’ feed to LSTM.

2ï¸âƒ£ LSTM processes sequence â†’ outputs hidden state `h_t`.

3ï¸âƒ£ Dense + Softmax layer gives probability distribution over vocabulary.

### Example output (top 3 predictions):

| Next word candidates | Probability |
| -------------------- | ----------- |
| eat                  | 0.7         |
| play                 | 0.2         |
| run                  | 0.1         |

âš ï¸ **Note:** The model produces a full probability vector across the entire vocabulary. We often show top N words for readability or use in decoding strategies.

---

## âœ… Example Code (Keras)

``python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=64),
    LSTM(128),
    Dense(vocab_size, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam')
``

---

## âš¡ Summary

* **LSTM** remembers long-term context using gates.
* Outputs hidden states `h_t` that represent the sequence so far.
* A softmax layer predicts next word probability distribution.
* We can use the top prediction or sample from top-k candidates for next-word generation.

---

## ğŸ“‚ Example usage

``python
input_text = ['I', 'love', 'to']
# Convert to indices or embeddings â†’ feed to model â†’ get softmax output
# Pick word with highest probability or sample
``

--

## Future improvements



--


